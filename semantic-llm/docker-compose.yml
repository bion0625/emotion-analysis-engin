# 모델 다운 명령어

# window gpu 설치 나중에 설정
# 리눅스 gpu 설치도 나중에 설정

# Google이 만든 경량 모델. 빠르고 효율적: 일단 이거 선택 but 추후에 (아래) 프랑스 걸로 수정
# docker compose up -d
# docker compose exec ollama ollama pull gemma:2b

# MADE IN 프랑스 -> 너무 느려서 추후에 GPU 설정 해야 할 거 같음
# 한국어에 특화된 Mistral-7B 기반 모델로, 한국어 QA, 요약, 생성, 추론 작업에 강점을 가짐.
# 한국어 멀티태스크 데이터로 SFT 및 DPO 튜닝되어 자연스러운 한글 응답 가능.
# Mistral의 경량성과 성능을 유지하면서도 한국어 성능을 대폭 향상시킨 모델.
# 코드, 논리, 수학 문제 등에서도 준수한 성능을 보이며, 노트북에서도 실행 가능.
# 아래 순서대로 작업
# docker compose up -d
# docker cp Modelfile semantic-llm-ollama-1:/root/Modelfile
# docker exec -it semantic-llm-ollama-1 ollama create komt-mistral -f /root/Modelfile
# 완료

# 모델들 저장소
# https://ollama.com/library

# ETC 모델들

# 기본
# docker compose up -d
# docker compose exec ollama ollama pull llama2
# 고도화
# docker compose up -d
# docker compose exec ollama ollama pull llama3

# MADE IN 한국
# LLaMA 2 기반으로 한국어·영어·코딩 작업에 균형 잡힌 성능을 가진 7B 모델.
# 한국어와 영어 instruction 튜닝이 되어 있어 계산, 코드, 대화, 추론 등 다양한 작업 가능.
# GSM8K 기준 수학 정확도 우수하며, 다국어 대응이 가능한 경량 멀티퍼포즈 모델.
# 노트북에서도 무리 없이 실행 가능하며, 실용성과 범용성을 모두 갖춘 모델.
# docker compose up -d
# docker compose exec ollama ollama pull kifai/gecko-7b

version: '3.8'

services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

volumes:
  ollama_data: